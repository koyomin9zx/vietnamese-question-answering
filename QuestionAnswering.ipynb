{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vietnamese Question Answering base on IR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    " - Window or Unix/Linux environment\n",
    " - Python 2.7\n",
    " - nltk\n",
    "\n",
    " - sklearn\n",
    " - googleapiclient : pip install --upgrade google-api-python-client\n",
    " - BeautifulSoup\n",
    " - plotly\n",
    " - underthesea\n",
    " - nltk\n",
    " \n",
    "# Originial Architecture\n",
    " - Question Answering based IR - Speech and language processing (daniel jurafsky)\n",
    " - https://web.stanford.edu/~jurafsky/slp3/24.pdf\n",
    " \n",
    "<img src=\"QA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library & Setting Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from googleapiclient.discovery import build\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import timeit\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import string\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "from nltk import sent_tokenize\n",
    "from underthesea import word_tokenize\n",
    "from underthesea import ner\n",
    "from collections import defaultdict\n",
    "\n",
    "Seach_api_key = \"AIzaSyCrmlMtMcJYVYSe731vyrVSAREKafE49Rk\"                    #Change this\n",
    "Custom_Search_Engine_ID = \"005336700654283051786:1mzldt1husk\"                #Change this\n",
    "    \n",
    "stopwords = set(open('stopwords.txt').read().decode('utf-8').split(' ')[:-1])\n",
    "puct_set = set([c for c in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'])\n",
    "\n",
    "def tokenize(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    sents = [word_tokenize(s,format = 'text') for s in sents]\n",
    "    return sents\n",
    "\n",
    "def get_entities(seq):\n",
    "    i = 0\n",
    "    chunks = []\n",
    "    seq = seq + ['O']  # add sentinel\n",
    "    types = [tag.split('-')[-1] for tag in seq]\n",
    "    while i < len(seq):\n",
    "        if seq[i].startswith('B'):\n",
    "            for j in range(i+1, len(seq)):\n",
    "                if seq[j].startswith('I') and types[j] == types[i]:\n",
    "                    continue\n",
    "                break\n",
    "            chunks.append((types[i], i, j))\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return chunks\n",
    "\n",
    "def _get_chunks(words, tags):\n",
    "    chunks = get_entities(tags)\n",
    "    res = defaultdict(list)\n",
    "    for chunk_type, chunk_start, chunk_end in chunks:\n",
    "        res[chunk_type].append(' '.join(words[chunk_start: chunk_end]))\n",
    "    return res\n",
    "\n",
    "def ner_extraction(text):\n",
    "    res = ner(text)\n",
    "    words = [r[0] for r in res]\n",
    "    tags = [t[3] for t in res]\n",
    "    \n",
    "    return _get_chunks(words,tags)\n",
    "\n",
    "def generateBigram(words):\n",
    "    bigrams = [words[i] + '_' + words[i+1] for i in range(0,len(words) - 1)]\n",
    "    return bigrams\n",
    "\n",
    "def noiseSent(sent):\n",
    "    if len(sent.split()) <= 3 or len(sent.split()) > 100:\n",
    "        return True\n",
    "    \n",
    "    if len(sent) <= 30:\n",
    "        return True\n",
    "    \n",
    "    if all(ord(c) < 128 for c in sent):\n",
    "        return True\n",
    "    \n",
    "    if not any(c.isalpha() for c in sent):\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Passage:\n",
    "    def __init__(self,string,rank,num_key):\n",
    "        self.sent = string            #sentences\n",
    "        self.ner = []                 #named entities\n",
    "        self.num_key = num_key        #number of match keywords\n",
    "        self.len_long_seq = 0         #length of longest exact sequence of question keywords\n",
    "        self.rank = rank              #rank of own document\n",
    "        self.ngram_overlap = 0        #ngram overlap question\n",
    "        self.proximity = 0            #shortest keywords that cover all keywords\n",
    "        self.score = 0                #Overall score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That is everything this system require . Time to do experiments : \n",
    "\n",
    "The QA system are able to answer all below type of question :\n",
    " - PERSON (PER)\n",
    " - LOCATION (LOC)\n",
    " - ORGANIZATION (ORG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input question & Keywords Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Người đầu tiên đặt chân lên mặt trăng ?\n",
      "Keywords : người , đầu_tiên , đặt_chân , mặt_trăng\n"
     ]
    }
   ],
   "source": [
    "query = u\"Người đầu tiên đặt chân lên mặt trăng ?\"\n",
    "#AnswerType = PER-ORG-LOC\n",
    "AnswerType = \"PER\"\n",
    "\n",
    "def keywords_extraction(sentences):\n",
    "    sent = sentences.lower()\n",
    "    sent = sent.split()\n",
    "    sent = [s for s in sent if s not in stopwords and s not in puct_set]\n",
    "    return sent\n",
    "\n",
    "token_query = tokenize(query)[0]\n",
    "keywords = keywords_extraction(token_query)\n",
    "print query\n",
    "print 'Keywords : ' + ' , '.join(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Relevant Document\n",
    " - Using google seach API to get relevant document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neil Armstrong – Wikipedia tiếng Việt\n",
      "https://vi.wikipedia.org/wiki/Neil_Armstrong\n",
      "Top 10 Sự Thật - Phi Hành Gia Amstrong Người Đầu Tiên Đặt Chân ...\n",
      "https://www.youtube.com/watch?v=BtF8s7yD9Jo\n",
      "Con người lần đầu đặt chân lên Mặt Trăng khi nào? - Tư vấn - Zing.vn\n",
      "https://news.zing.vn/con-nguoi-lan-dau-dat-chan-len-mat-trang-khi-nao-post838809.html\n",
      "Kỷ niệm 45 năm con người đặt chân lên Mặt Trăng - YouTube\n",
      "https://www.youtube.com/watch?v=esqt3QCrtxQ\n",
      "Lần đầu tiên con người đặt chân lên Mặt trăng | baotintuc.vn\n",
      "https://baotintuc.vn/giai-mat/lan-dau-tien-con-nguoi-dat-chan-len-mat-trang-20140716160619157.htm\n",
      "1.56707000732\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "service = build(\"customsearch\", \"v1\",developerKey=Seach_api_key)\n",
    "\n",
    "def ggsearch(i):\n",
    "    if (i == 0):\n",
    "        res = service.cse().list(q=query,cx = Custom_Search_Engine_ID).execute()\n",
    "    else:\n",
    "        res = service.cse().list(q=query,cx = Custom_Search_Engine_ID,num=10,start = i*10).execute()\n",
    "    return res[u'items']\n",
    "    \n",
    "pool = Pool(4)\n",
    "pages_content = pool.map(ggsearch,range(3))\n",
    "pool.terminate()\n",
    "\n",
    "pages_content = [j for i in pages_content for j in i]\n",
    "\n",
    "document_urls = []\n",
    "document_titles = []\n",
    "for page in pages_content:\n",
    "    if 'fileFormat' in page:\n",
    "        print 'Skip ' +  page[u'link']\n",
    "        continue\n",
    "    document_urls.append(page[u'link'])\n",
    "    document_titles.append(page[u'title'])\n",
    "    \n",
    "for i in range(0,5):\n",
    "    print document_titles[i]\n",
    "    print document_urls[i]\n",
    "    \n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passage Retrieval\n",
    " - Get all sentences from all document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all candidate passages from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = []\n",
    "total_start = time.time()\n",
    "    \n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def getContent(para):\n",
    "    url = para[0]\n",
    "    rank = int((para[1] + 10)/10) - 1 \n",
    "    passages = []\n",
    "    try:\n",
    "        html = requests.get(url, timeout = 5)\n",
    "    except:\n",
    "        print 'Cannot read ' + url\n",
    "        return []\n",
    "    \n",
    "    tree = BeautifulSoup(html.text,'lxml')\n",
    "    for invisible_elem in tree.find_all(['script', 'style']):\n",
    "        invisible_elem.extract()\n",
    "    \n",
    "    sents = []\n",
    "    text_chunks = list(chunks(tree.get_text(),100000))\n",
    "    for text in text_chunks:\n",
    "        sents += tokenize(text)\n",
    "    \n",
    "    for sent in sents:\n",
    "        sent = sent.strip()\n",
    "        if not noiseSent(sent):\n",
    "            sent_keywords = keywords_extraction(sent)\n",
    "            num_overlap_keywords = len(set(sent_keywords) & set(keywords))\n",
    "            if num_overlap_keywords > 0:\n",
    "                passages.append(Passage(sent,rank,num_overlap_keywords))\n",
    "                \n",
    "    return passages\n",
    "\n",
    "pool = Pool(20)\n",
    "passages = pool.map(getContent,[(document_urls[i],i) for i in range(0,len(document_urls))])\n",
    "pool.terminate()\n",
    "passages = [j for i in passages for j in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition,  Eliminate passages have no entity match answer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passages : 323\n",
      "After Filtering : 212\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(passages)):\n",
    "    passages[i].ner = list(set(ner_extraction(passages[i].sent)[AnswerType]))\n",
    "\n",
    "print 'Number of passages : ' + str(len(passages))\n",
    "passages = [p for p in passages if len(p.ner) > 0]\n",
    "print 'After Filtering : ' + str(len(passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời_khắc lịch_sử , Armstrong và Buzz Aldrin đặt_chân lên bề_mặt Mặt_Trăng và dành 2,5 giờ khám_phá trong_khi Michael Collins ở lại trên quỹ_đạo trong Module Command .\n",
      "Module Command # Armstrong\n",
      "\n",
      "\n",
      "Năm 1969 , Armstrong nhận nhiệm_vụ tham_gia chuyến bay Apollo 11 và sứ_mệnh đại_diện cho cả ngành hàng_không vũ_trụ Mỹ trong việc đặt_chân lên Mặt_Trăng .\n",
      "Armstrong\n",
      "\n",
      "\n",
      "Armstrong bước trên mặt_trăng , 20 tháng 7 năm 1969 .\n",
      "Armstrong\n",
      "\n",
      "\n",
      "\" Đây là bước_đi nhỏ của một con_người , nhưng là bước_tiến lớn của nhân_loại \" Sáng_sớm ngày 20/7/1969 , Armstrong trở_thành người đầu_tiên đặt_chân xuống Mặt_Trăng với câu_nói nổi_tiếng : \" Đây là bước_đi nhỏ của một con_người , nhưng là bước_tiến lớn của nhân_loại \" .\n",
      "Armstrong\n",
      "\n",
      "\n",
      "Mặc_dù từng lái máy_bay chiến_đấu cho hải_quân Mỹ , làm phi_công thử_nghiệm và phi_hành gia cho Cơ_quan Hàng_không vũ_trụ Mỹ ( NASA ) , Armstrong chưa bao_giờ cho_phép bản_thân chìm_đắm trong ánh hào_quang sau chuyến bay lên Mặt_Trăng vào năm 1969 .\n",
      "Cơ_quan Hàng_không # Mặc_dù # Armstrong\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in passages[:5]:\n",
    "    print p.sent\n",
    "    print ' # '.join(p.ner)\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Passages by number of keyword\n",
    "\n",
    " - Find the maximum number of question keyword contain in a passages\n",
    " - Keep passages have number of question keyword < MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total passages : 212\n",
      "Max number of question keyword : 4\n",
      "After filtering : 33\n",
      "\n",
      "0 - \" Đây là bước_đi nhỏ của một con_người , nhưng là bước_tiến lớn của nhân_loại \" Sáng_sớm ngày 20/7/1969 , Armstrong trở_thành người đầu_tiên đặt_chân xuống Mặt_Trăng với câu_nói nổi_tiếng : \" Đây là bước_đi nhỏ của một con_người , nhưng là bước_tiến lớn của nhân_loại \" .\n",
      "\n",
      "1 - Người_ta sẽ nói rằng ông ấy là người đầu_tiên đặt_chân lên một thiên_thể ngoài Trái_Đất \" Gia_đình Armstrong tuyên_bố trong cáo_phó trong đám_tang ông : \" Neil Armstrong là một vị anh_hùng bất_đắc_dĩ , bởi ông ấy nghĩ rằng đặt_chân lên Mặt_Trăng chỉ là một phần của_công_việc \" Tham_khảo [ sửa | sửa mã nguồn ] Chú_thích ^ “ Neil Armstrong ' s Death — a Medical Perspective ” .\n",
      "\n",
      "2 - Top 10 Sự_Thật - Phi_Hành Gia Amstrong Người Đầu_Tiên Đặt_Chân Lên Mặt_Trăng - YouTube Bỏ_qua điều hướng VN Đăng nhập Tìm_kiếm Đang tải ... Chọn ngôn_ngữ của bạn .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Total passages : ' +  str(len(passages))\n",
    "max_keyword = 0\n",
    "min_num_passages = 20\n",
    "for p in passages:\n",
    "    if p.num_key > max_keyword:\n",
    "        max_keyword = p.num_key\n",
    "        \n",
    "while (True):\n",
    "    num_candidate_passages = 0\n",
    "    for p in passages:\n",
    "        if p.num_key >= max_keyword:\n",
    "            num_candidate_passages += 1\n",
    "    if (num_candidate_passages >= min_num_passages or max_keyword == 1):\n",
    "        break\n",
    "    else:\n",
    "        max_keyword -=1\n",
    "        \n",
    "print 'Max number of question keyword : ' + str(max_keyword)\n",
    "passages = [p for p in passages if p.num_key >= max_keyword]\n",
    "print 'After filtering : ' +  str(len(passages)) + '\\n'\n",
    "for i in range(0,min(3,len(passages))):\n",
    "    print str(i) + ' - ' + passages[i].sent + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Passages\n",
    "- number of words\n",
    "- number of named entities\n",
    "- number of keywords\n",
    "- length of longest exact sequence of question keywords\n",
    "- rank of own document\n",
    "- ngram overlap question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(passages)):\n",
    "    score = 3\n",
    "    score -= passages[i].rank\n",
    "    score -= len(passages[i].ner)\n",
    "    score += passages[i].num_key\n",
    "    score -= int(len(passages[i].sent.split()) / 50.0)\n",
    "    \n",
    "    x = token_query.lower().split()\n",
    "    y = p.sent.lower().split()\n",
    "    s = SequenceMatcher(None, x, y)\n",
    "    score += s.find_longest_match(0, len(x), 0, len(y)).size\n",
    "    \n",
    "    bigram_q = generateBigram(x)\n",
    "    bigram_p = generateBigram(y)\n",
    "    score += len(set(bigram_q) & set(bigram_p))\n",
    "    \n",
    "    passages[i].score = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Extraction\n",
    "\n",
    " - Correct answer is the entity with highest score\n",
    " - Answer score = Sum( Score of passages that contain the answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neil Armstrong 55.17 %\n",
      "Armstrong 16.38 %\n",
      "Buzz Aldrin 11.21 %\n",
      "Phi_Hành Gia 9.48 %\n",
      "Chắc_hẳn 7.76 %\n",
      "Video 7.76 %\n",
      "Câu_hỏi 7.76 %\n",
      "Michael Collins 6.9 %\n",
      "ĐẶT_CHÂN LÊN MẶT_TRĂNG 5.17 %\n",
      "Xếp_hạng 5.17 %\n"
     ]
    }
   ],
   "source": [
    "candidates = [p.ner for p in passages]\n",
    "candidates = list(set([j for i in candidates for j in i]))\n",
    "candidates = [(c,0) for c in candidates]\n",
    "candidates = dict(candidates)\n",
    "\n",
    "for p in passages:\n",
    "    for ner in p.ner:\n",
    "        candidates[ner] += p.score\n",
    "\n",
    "candidates = candidates.items()\n",
    "\n",
    "candidates = sorted(candidates, key = lambda x: x[1],reverse = True)\n",
    "candidates = candidates[:10]\n",
    "total_score = float(sum([c[1] for c in candidates[:5]]))\n",
    "\n",
    "for c in candidates:\n",
    "    print c[0], round((c[1] / total_score) * 100,2), \"%\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
